# RAG Model Selector

<p align="center">
  <img src="https://img.shields.io/badge/RAG-Model_Selector-blue?style=for-the-badge" alt="RAG Model Selector"/>
</p>

<h3 align="center">Find the Perfect LLM for Your RAG System</h3>

<p align="center">
  <strong>Enterprise-Grade Benchmarking Tool for AI Model Selection</strong><br>
  <em>Help individuals and enterprises discover which AI model performs best with their specific data</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/Streamlit-1.28+-FF4B4B?style=flat-square&logo=streamlit&logoColor=white" alt="Streamlit"/>
  <img src="https://img.shields.io/badge/LangChain-0.3+-1C3C3C?style=flat-square&logo=chainlink&logoColor=white" alt="LangChain"/>
  <img src="https://img.shields.io/badge/FAISS-Vector_DB-00ADD8?style=flat-square&logo=meta&logoColor=white" alt="FAISS"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square" alt="License"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/API-Gemini_|_GPT_|_Claude-8B5CF6?style=flat-square" alt="API Models"/>
  <img src="https://img.shields.io/badge/Local-Llama_|_Mistral_|_Phi_|_Qwen-F97316?style=flat-square" alt="Local Models"/>
  <img src="https://img.shields.io/badge/Metrics-Semantic_|_BERT_|_ROUGE-10B981?style=flat-square" alt="Metrics"/>
</p>

---

## Contents

- [About the Project](#about-the-project)
- [Why RAG Model Selector](#why-rag-model-selector)
- [Features](#features)
- [Technologies](#technologies)
- [System Architecture](#system-architecture)
- [Project Structure](#-project-structure)
- [Installation](#%EF%B8%8F-installation)
- [Usage Guide](#usage-guide)
- [Evaluation Metrics](#-evaluation-metrics)
- [Sample Results](#-sample-results)
- [Configuration](#-configuration)
- [Contributing](#-contributing)
- [License](#-license)

---

## About the Project

**RAG Model Selector** is a benchmarking platform that modernizes the LLM selection process for RAG (Retrieval-Augmented Generation) systems. By leveraging scientific evaluation metrics, it helps users identify the optimal model for their specific use case.

Building a RAG system requires choosing the right LLM, but:
- **Which model works best with YOUR specific data?**
- **Should you use expensive API models or free local alternatives?**
- **How do models compare on accuracy vs. speed vs. cost?**

This tool answers these questions with data-driven insights.

---

## Why RAG Model Selector

<table>
<tr>
<td width="50%">

### Scientific Evaluation
Three-metric weighted scoring system ensures comprehensive and unbiased model assessment using Semantic Similarity, BERTScore, and ROUGE metrics.

</td>
<td width="50%">

### Multi-Model Support
Test 7 different LLMs simultaneously - including Google Gemini, OpenAI GPT, Anthropic Claude, and local models via Ollama.

</td>
</tr>
<tr>
<td width="50%">

### Interactive Visualizations
Charts including bar graphs, radar charts, and delta analysis to visualize model performance across multiple dimensions.

</td>
<td width="50%">

### AI-Powered Insights
Claude automatically analyzes benchmark results and provides personalized recommendations for your use case.

</td>
</tr>
<tr>
<td width="50%">

### Performance Optimized
GPU acceleration, vectorstore caching, and sequential execution ensure fast benchmarks without system overload.

</td>
<td width="50%">

### Cost-Aware Analysis
Track token usage and API costs in real-time, helping you make budget-conscious decisions.

</td>
</tr>
</table>

---

## Features

### For Individual Developers

| Feature | Description |
|---------|-------------|
| **Model Comparison** | Compare API and local models on your own data |
| **Cost Optimization** | Find free local alternatives to expensive API models |
| **Quick Testing** | Test with 8-64 questions for fast results |
| **Export Results** | Download benchmark results as CSV for further analysis |

### For Enterprises

| Feature | Description |
|---------|-------------|
| **Data Privacy** | Test with local models - your data never leaves your server |
| **Scalable Testing** | Benchmark multiple models systematically |
| **Compliance Ready** | Document model selection decisions with scientific metrics |
| **Team Collaboration** | Share reproducible benchmark results |

### Evaluation Capabilities

| Capability | Description |
|------------|-------------|
| **Semantic Similarity** | Embedding-based meaning comparison (60% weight) |
| **BERTScore** | Context-aware token matching - catches synonyms (30% weight) |
| **ROUGE-L** | N-gram overlap for factual accuracy (10% weight) |
| **Hardware Monitoring** | Real-time RAM/CPU tracking with safety thresholds |

---

## Technologies

<table>
<tr>
<td align="center" width="96">
  <img src="https://skillicons.dev/icons?i=python" width="48" height="48" alt="Python" />
  <br>Python
</td>
<td align="center" width="96">
  <img src="https://streamlit.io/images/brand/streamlit-mark-color.svg" width="48" height="48" alt="Streamlit" />
  <br>Streamlit
</td>
<td align="center" width="96">
  <img src="https://skillicons.dev/icons?i=pytorch" width="48" height="48" alt="PyTorch" />
  <br>PyTorch
</td>
<td align="center" width="96">
  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="48" height="48" alt="HuggingFace" />
  <br>HuggingFace
</td>
<td align="center" width="96">
  <img src="https://skillicons.dev/icons?i=github" width="48" height="48" alt="GitHub" />
  <br>GitHub
</td>
</tr>
</table>

### Backend & Core

| Technology | Purpose |
|------------|---------|
| **Python 3.8+** | Core programming language |
| **LangChain 0.3+** | RAG framework and model orchestration |
| **FAISS** | Vector similarity search |
| **Sentence Transformers** | Text embeddings |
| **psutil** | Hardware monitoring |

### Frontend & Visualization

| Technology | Purpose |
|------------|---------|
| **Streamlit** | Interactive web UI |
| **Plotly** | Interactive charts (bar, radar) |
| **Pandas** | Data manipulation and export |

### LLM Providers

| Provider | Models | Type |
|----------|--------|------|
| **Google** | Gemini 2.5 Flash | API |
| **OpenAI** | GPT-3.5 Turbo | API |
| **Anthropic** | Claude 3.5 Haiku | API |
| **Ollama** | Llama 3.1, Mistral, Phi-3, Qwen 2 | Local |

### Evaluation Models

| Model | Purpose |
|-------|---------|
| **all-MiniLM-L6-v2** | Shared embedding (Scenario 1) |
| **BGE-Large-EN** | Llama-specific embedding |
| **MPNet-Base** | Mistral-specific embedding |
| **Multilingual-MiniLM** | Phi-specific embedding |
| **BGE-Base-EN** | Qwen-specific embedding |
| **bert-base-multilingual-cased** | BERTScore evaluation |

---

## System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        RAG MODEL SELECTOR                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Corpus CSV ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Chunking   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  FAISS Vectorstore      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (Your Data)‚îÇ    ‚îÇ  (1000 chr) ‚îÇ    ‚îÇ  (GPU Accelerated)      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                     ‚îÇ               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Test CSV   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Questions  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Retriever (Top-K)      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (Q&A Pairs)‚îÇ    ‚îÇ  Iteration  ‚îÇ    ‚îÇ  Context Generation     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                     ‚îÇ               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                    LLM GENERATION LAYER                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Gemini  ‚îÇ  ‚îÇ   GPT   ‚îÇ  ‚îÇ Claude  ‚îÇ  ‚îÇ  Ollama (Local)     ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  API    ‚îÇ  ‚îÇ   API   ‚îÇ  ‚îÇ   API   ‚îÇ  ‚îÇ Llama|Mistral|Phi|Q ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                               ‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                   UNIFIED EVALUATOR                            ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Semantic    ‚îÇ  ‚îÇ  BERTScore   ‚îÇ  ‚îÇ   ROUGE-L    ‚îÇ          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (60%)       ‚îÇ  ‚îÇ  (30%)       ‚îÇ  ‚îÇ   (10%)      ‚îÇ          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                               ‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                 RESULTS & VISUALIZATION                        ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Performance Tables    Interactive Charts    AI Analysis       ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
User Data ‚Üí Chunking ‚Üí Embedding ‚Üí FAISS Index ‚Üí Retrieval ‚Üí LLM ‚Üí Evaluation ‚Üí Results
```

---

## üìÅ Project Structure

```
RAG-Model-Selector/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Configuration exports
‚îÇ   ‚îî‚îÄ‚îÄ model_config.py          # Model configs, pricing, embeddings
‚îÇ
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Core module exports
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_runner.py      # Test orchestration engine
‚îÇ   ‚îú‚îÄ‚îÄ csv_processor.py         # Data ingestion & chunking
‚îÇ   ‚îú‚îÄ‚îÄ hardware_monitor.py      # RAM/CPU monitoring & safety
‚îÇ   ‚îú‚îÄ‚îÄ model_manager.py         # LLM loading & invocation
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py          # FAISS vectorstore management
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py          # Evaluation exports
‚îÇ       ‚îú‚îÄ‚îÄ unified_evaluator.py # Weighted score aggregation
‚îÇ       ‚îú‚îÄ‚îÄ semantic_scorer.py   # Sentence Transformers similarity
‚îÇ       ‚îú‚îÄ‚îÄ bert_scorer.py       # BERTScore (P/R/F1)
‚îÇ       ‚îú‚îÄ‚îÄ rouge_scorer.py      # ROUGE-1, ROUGE-2, ROUGE-L
‚îÇ       ‚îî‚îÄ‚îÄ keyword_scorer.py    # Keyword extraction & matching
‚îÇ
‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îî‚îÄ‚îÄ vectorstores/            # Cached FAISS indices (gitignored)
‚îÇ
‚îú‚îÄ‚îÄ enhanced_app.py              # Streamlit UI application
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ env.example                  # API key template
‚îú‚îÄ‚îÄ .gitignore                   # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE                      # MIT License
‚îî‚îÄ‚îÄ README.md                    # This file
```

---

## ‚öôÔ∏è Installation

### Prerequisites

- **Python 3.8+**
- **Ollama** (for local models) - [Download](https://ollama.ai/)
- **CUDA-compatible GPU** (optional, but recommended)

### Step 1: Clone the Repository

```bash
git clone https://github.com/utkuakbay/RAG_Benchmark.git
cd RAG_Benchmark
```

### Step 2: Create Virtual Environment

```bash
# Windows
python -m venv venv
.\venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Configure API Keys

```bash
# Copy example file
cp env.example .env

# Edit .env with your API keys
# GOOGLE_API_KEY=your_key_here
# OPENAI_API_KEY=your_key_here
# ANTHROPIC_API_KEY=your_key_here
```

### Step 5: Download Local Models (Optional)

```bash
# Start Ollama service
ollama serve

# Download models (in a new terminal)
ollama pull llama3.1:8b      # Meta Llama 3.1
ollama pull mistral:7b       # Mistral 7B
ollama pull phi3:mini        # Microsoft Phi-3
ollama pull qwen2:7b         # Alibaba Qwen 2
```

### Step 6: Run the Application

```bash
streamlit run enhanced_app.py
```

The application will open at `http://localhost:8501`

---

## Usage Guide

### Step 1: Prepare Your Data

**Corpus CSV** (your knowledge base):
```csv
title,content,category
Product Overview,Our software provides real-time analytics...,Documentation
Installation Guide,To install the application follow these steps...,Technical
FAQ,Common questions include how to reset password...,Support
```

**Test CSV** (evaluation questions with ideal answers):
```csv
soru,ideal_cevap
What does the software do?,The software provides real-time analytics and reporting.
How do I install it?,Follow the installation guide in the documentation.
```

### Step 2: Configure Benchmark

| Parameter | Range | Default | Description |
|-----------|-------|---------|-------------|
| **Test Questions** | 8-64 | 16 | Number of evaluation questions |
| **Chunk Size** | 500-2000 | 1000 | Characters per document chunk |
| **Chunk Overlap** | 0-500 | 200 | Overlap between chunks |
| **Retriever K** | 1-10 | 3 | Documents retrieved per query |

### Step 3: Select Models & Scenarios

**Scenario 1 (Fair Arena):**
- All models use the same embedding (MiniLM-L6-v2)
- Fair comparison of LLM capabilities only

**Scenario 2 (Real World):**
- Each local model uses its optimized embedding
- Simulates production performance

### Step 4: Run & Analyze

1. Click **"Benchmark Ba≈ülat"** (Start Benchmark)
2. Monitor progress in real-time
3. Review performance tables
4. Explore interactive charts
5. Read AI-generated recommendations
6. Export results as CSV

---

## üìä Evaluation Metrics

### Scoring Formula

```
Final Score = (Semantic √ó 0.60) + (BERT √ó 0.30) + (ROUGE √ó 0.10)
```

### Metric Details

| Metric | Weight | What It Measures | Example |
|--------|--------|------------------|---------|
| **Semantic Similarity** | 60% | Meaning-based comparison via embeddings | "car" ‚âà "automobile" |
| **BERTScore** | 30% | Context-aware token matching | "doctor" ‚âà "physician" |
| **ROUGE-L** | 10% | Longest common subsequence | Exact phrase matching |

### Why These Weights?

- **Semantic (60%)**: Primary metric - captures meaning even with different words
- **BERT (30%)**: Secondary - adds contextual understanding
- **ROUGE (10%)**: Tertiary - rewards factual accuracy but penalizes paraphrasing

---

## üìà Sample Results

### Benchmark Results (8 Questions, Chunk Size: 1000, K: 3)

| Model | Score | Semantic | BERT | ROUGE | Time | Cost |
|-------|-------|----------|------|-------|------|------|
| **Claude 3.5 Haiku** | **84.9** | 87.9 | 84.9 | 66.7 | 1.66s | $0.0007 |
| Qwen 2 (7B) | 81.9 | 83.1 | 83.6 | 69.6 | 2.15s | $0.0000 |
| Mistral (7B) | 76.2 | 79.5 | 78.4 | 49.6 | 3.63s | $0.0000 |
| Gemini 2.5 Flash | 71.6 | 73.0 | 73.3 | 58.2 | 1.53s | $0.0001 |
| Llama 3.1 (8B) | 68.8 | 67.0 | 74.3 | 36.6 | 2.09s | $0.0000 |
| Phi-3 (3.8B) | 56.0 | 58.8 | 63.6 | 16.7 | 6.94s | $0.0000 |

### Key Findings

- **Best Overall:** Claude 3.5 Haiku - highest scores with fast response times
- **Best Value:** Qwen 2 - 96% of Claude's performance at zero cost
- **Fastest:** Gemini 2.5 Flash - 1.53s average response time
- **Not Recommended:** Phi-3 - lower scores with slower responses

> **Note:** Results vary based on your dataset, chunk size, number of questions, and retriever K value. The quality and domain-specificity of your corpus and test questions significantly impact model performance. We recommend running multiple benchmarks with different configurations to find the optimal setup for your use case.

---

## üí∞ Cost Estimation

| Model | Input Price | Output Price | ~Cost per 100 Queries |
|-------|-------------|--------------|----------------------|
| Gemini 2.5 Flash | $0.075/1M | $0.30/1M | ~$0.02 |
| GPT-3.5 Turbo | $0.50/1M | $1.50/1M | ~$0.10 |
| Claude 3.5 Haiku | $0.25/1M | $1.25/1M | ~$0.08 |
| **Local Models** | **Free** | **Free** | **$0.00** |

---

## üîß Configuration

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GOOGLE_API_KEY` | Optional | For Gemini models |
| `OPENAI_API_KEY` | Optional | For GPT models |
| `ANTHROPIC_API_KEY` | Optional | For Claude models |

### Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **RAM** | 8 GB | 16+ GB |
| **GPU** | None | CUDA-compatible |
| **Storage** | 5 GB | 10+ GB (for local models) |

---

## ü§ù Contributing

Contributions are welcome! Follow these steps:

1. **Fork** the repository
2. **Create** a feature branch
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. **Commit** your changes
   ```bash
   git commit -m 'Add some AmazingFeature'
   ```
4. **Push** to the branch
   ```bash
   git push origin feature/AmazingFeature
   ```
5. **Open** a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Add docstrings to all functions
- Write unit tests for new features
- Update README for significant changes

---

## üìÑ License

```
MIT License

Copyright (c) 2026 Mustafa Utku Akbay

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
```

---

<p align="center">
  <strong>Stop guessing. Start benchmarking.</strong><br>
  <em>Find the perfect LLM for your RAG system.</em>
</p>

<p align="center">
  <a href="https://github.com/utkuakbay/RAG_Benchmark">
    <img src="https://img.shields.io/badge/Made%20with-%E2%9D%A4%EF%B8%8F-white?style=for-the-badge&labelColor=black" alt="Made with love"/>
  </a>
</p>
