"""
Enhanced RAG Benchmark System - Streamlit UI

Multi-Model RAG Benchmark:
- API Modeller: Gemini, GPT, Claude
- Local Modeller (Ollama): Llama, Mistral, Phi, Qwen

Ä°ki Senaryo:
1. Shared Embedding (Fair Arena)
2. Model-Specific Embedding (Real World)
"""

import streamlit as st
import pandas as pd
import time
import os
from typing import Optional
import plotly.express as px
import plotly.graph_objects as go

from dotenv import load_dotenv

# Core imports
from core.csv_processor import CSVProcessor
from core.rag_pipeline import RAGPipeline
from core.model_manager import ModelManager
from core.benchmark_runner import BenchmarkRunner
from core.hardware_monitor import HardwareMonitor
from core.evaluation.unified_evaluator import UnifiedEvaluator

# Config imports
from config.model_config import MODEL_CONFIG, HARDWARE_CONFIG

# Load environment variables
load_dotenv()


def init_session_state():
    """Session state'i baÅŸlat."""
    if 'benchmark_results' not in st.session_state:
        st.session_state.benchmark_results = None
    if 'test_completed' not in st.session_state:
        st.session_state.test_completed = False
    if 'hw_monitor' not in st.session_state:
        st.session_state.hw_monitor = HardwareMonitor()


def display_system_status():
    """Sistem durumunu gÃ¶ster."""
    hw_monitor = st.session_state.hw_monitor
    stats = hw_monitor.get_system_stats()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Renk gÃ¶stergeleri fonksiyonel - kalÄ±yor
        ram_color = "ðŸŸ¢" if stats.ram_percent < 70 else "ðŸŸ¡" if stats.ram_percent < 85 else "ðŸ”´"
        st.metric(
            f"{ram_color} RAM KullanÄ±mÄ±", 
            f"{stats.ram_percent:.1f}%",
            f"{stats.ram_used_gb:.1f} / {stats.ram_total_gb:.1f} GB"
        )
    
    with col2:
        cpu_color = "ðŸŸ¢" if stats.cpu_percent < 70 else "ðŸŸ¡" if stats.cpu_percent < 85 else "ðŸ”´"
        st.metric(
            f"{cpu_color} CPU KullanÄ±mÄ±",
            f"{stats.cpu_percent:.1f}%"
        )
    
    with col3:
        st.metric(
            "KullanÄ±labilir RAM",
            f"{stats.ram_available_gb:.1f} GB"
        )
    
    # UyarÄ±lar - fonksiyonel
    if stats.ram_percent > 85:
        st.warning("RAM kullanÄ±mÄ± yÃ¼ksek! Test sÄ±rasÄ±nda sorun yaÅŸanabilir.")
    elif stats.ram_available_gb < 4:
        st.warning("KullanÄ±labilir RAM dÃ¼ÅŸÃ¼k. DiÄŸer uygulamalarÄ± kapatmanÄ±zÄ± Ã¶neririz.")


def display_ollama_status(model_manager: ModelManager):
    """Ollama durumunu gÃ¶ster."""
    if model_manager.ollama_available:
        st.success("Ollama servisi Ã§alÄ±ÅŸÄ±yor")
    else:
        st.error("""
        Ollama servisi bulunamadÄ±!
        
        Ã‡Ã¶zÃ¼m:
        1. Ollama'nÄ±n kurulu olduÄŸundan emin olun
        2. PowerShell'de `ollama serve` komutunu Ã§alÄ±ÅŸtÄ±rÄ±n
        3. SayfayÄ± yenileyin
        """)


def display_results_table(summary_df: pd.DataFrame):
    """SonuÃ§ tablosunu gÃ¶ster."""
    if summary_df.empty:
        st.warning("SonuÃ§ bulunamadÄ±.")
        return
    
    # Metrik aÃ§Ä±klamalarÄ±
    with st.expander("â„¹ï¸ Metrikler nasÄ±l hesaplanÄ±yor?", expanded=False):
        st.markdown("""
        ### DeÄŸerlendirme FormÃ¼lÃ¼
        ```
        Avg Score = (Semantic Ã— 0.60) + (BERT Ã— 0.30) + (ROUGE Ã— 0.10)
        ```
        
        ---
        
        | Metrik | AÄŸÄ±rlÄ±k | AÃ§Ä±klama |
        |--------|---------|----------|
        | **Avg Semantic** | 60% | Anlamsal benzerlik. Cevap ve ideal cevap vektÃ¶re Ã§evrilip cosine similarity hesaplanÄ±r. Kelimeler farklÄ± olsa bile anlamÄ± yakalar. |
        | **Avg BERT** | 30% | BERTScore. Her kelime baÄŸlamsal embedding'e Ã§evrilir. EÅŸ anlamlÄ± kelimeleri (doktorâ†”hekim) tanÄ±r. |
        | **Avg ROUGE** | 10% | ROUGE-L. En uzun ortak kelime dizisini bulur. Sadece kelime eÅŸleÅŸmesi, anlam yakalamaz. |
        
        ---
        
        | DiÄŸer Metrikler | AÃ§Ä±klama |
        |-----------------|----------|
        | **Avg Time (s)** | Model baÅŸÄ±na ortalama yanÄ±t sÃ¼resi |
        | **Total Tokens** | Toplam kullanÄ±lan token sayÄ±sÄ± |
        | **Total Cost ($)** | API modelleri iÃ§in tahmini maliyet |
        | **Avg RAM (%)** | Test sÄ±rasÄ±ndaki ortalama RAM kullanÄ±mÄ± |
        | **Errors** | BaÅŸarÄ±sÄ±z olan soru sayÄ±sÄ± |
        """)
    
    # Formatla
    display_df = summary_df.copy()
    
    # SayÄ±sal sÃ¼tunlarÄ± formatla
    for col in ['Avg Score', 'Avg Semantic', 'Avg BERT', 'Avg ROUGE']:
        if col in display_df.columns:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.1f}")
    
    if 'Avg Time (s)' in display_df.columns:
        display_df['Avg Time (s)'] = display_df['Avg Time (s)'].apply(lambda x: f"{x:.2f}")
    
    if 'Total Cost ($)' in display_df.columns:
        display_df['Total Cost ($)'] = display_df['Total Cost ($)'].apply(lambda x: f"${x:.6f}")
    
    if 'Avg RAM (%)' in display_df.columns:
        display_df['Avg RAM (%)'] = display_df['Avg RAM (%)'].apply(lambda x: f"{x:.1f}%")
    
    st.dataframe(display_df, width='stretch')


def display_comparison_table(comparison_df: pd.DataFrame):
    """Delta karÅŸÄ±laÅŸtÄ±rma tablosunu gÃ¶ster."""
    if comparison_df.empty:
        st.info("KarÅŸÄ±laÅŸtÄ±rma iÃ§in her iki senaryo da Ã§alÄ±ÅŸtÄ±rÄ±lmalÄ±.")
        return
    
    st.subheader("Scenario 1 vs Scenario 2 â€” Delta Analizi")
    
    # Formatla
    display_df = comparison_df.copy()
    
    for col in ['Shared Score', 'Own Score', 'Score Delta']:
        if col in display_df.columns:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.1f}")
    
    for col in ['Shared Time', 'Own Time', 'Time Delta']:
        if col in display_df.columns:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.2f}s")
    
    st.dataframe(display_df, width='stretch')
    
    # Ã–zet
    improved = len(comparison_df[comparison_df['Improvement'] == 'Yes'])
    total = len(comparison_df)
    
    if improved > 0:
        st.success(f"{improved}/{total} modelde kendi embedding'i ile performans artÄ±ÅŸÄ± gÃ¶rÃ¼ldÃ¼.")
    else:
        st.info("Shared embedding tÃ¼m modellerde yeterli performans saÄŸlÄ±yor.")


def display_charts(summary_df: pd.DataFrame):
    """Benchmark sonuÃ§larÄ± iÃ§in grafikler gÃ¶ster."""
    if summary_df.empty:
        return
    
    # Renk paleti (dark tema uyumlu)
    colors = {
        'primary': '#F5A623',      # Turuncu/AltÄ±n
        'secondary': '#4ECDC4',    # Turkuaz
        'success': '#2ECC71',      # YeÅŸil
        'warning': '#E74C3C',      # KÄ±rmÄ±zÄ±
        'background': '#0E1117',   # Dark background
        'text': '#FAFAFA'          # Light text
    }
    
    # Grafik layout ayarlarÄ±
    layout_config = dict(
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        font=dict(color=colors['text'], size=12),
        margin=dict(l=40, r=40, t=60, b=40),
        height=400
    )
    
    st.subheader("Performans Grafikleri")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Skor GrafiÄŸi
        fig_score = go.Figure()
        
        # Scenario 1 ve 2'yi ayÄ±r
        s1_data = summary_df[summary_df['Scenario'] == 1].copy()
        s2_data = summary_df[summary_df['Scenario'] == 2].copy()
        
        if not s1_data.empty:
            score_values_s1 = pd.to_numeric(s1_data['Avg Score'], errors='coerce')
            fig_score.add_trace(go.Bar(
                name='Scenario 1',
                x=s1_data['Model'],
                y=score_values_s1,
                marker_color=colors['primary'],
                text=score_values_s1.round(1),
                textposition='outside',
                textfont=dict(size=11),
                hovertemplate='%{x}<br>Skor: %{y:.1f}<extra></extra>'
            ))
        
        if not s2_data.empty:
            score_values_s2 = pd.to_numeric(s2_data['Avg Score'], errors='coerce')
            fig_score.add_trace(go.Bar(
                name='Scenario 2',
                x=s2_data['Model'],
                y=score_values_s2,
                marker_color=colors['secondary'],
                text=score_values_s2.round(1),
                textposition='outside',
                textfont=dict(size=11),
                hovertemplate='%{x}<br>Skor: %{y:.1f}<extra></extra>'
            ))
        
        fig_score.update_layout(
            title=dict(text='Model Performans SkorlarÄ±', font=dict(size=16)),
            xaxis_title='',
            yaxis_title='Skor',
            barmode='group',
            yaxis=dict(range=[0, 105], gridcolor='rgba(255,255,255,0.1)'),
            xaxis=dict(gridcolor='rgba(255,255,255,0.1)'),
            legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),
            **layout_config
        )
        
        st.plotly_chart(fig_score, key="chart_score")
    
    with col2:
        # SÃ¼re GrafiÄŸi
        fig_time = go.Figure()
        
        # SÃ¼re verilerini sayÄ±sal olarak al
        if not s1_data.empty:
            time_values_s1 = pd.to_numeric(s1_data['Avg Time (s)'], errors='coerce')
            fig_time.add_trace(go.Bar(
                name='Scenario 1',
                x=s1_data['Model'],
                y=time_values_s1,
                marker_color=colors['primary'],
                text=time_values_s1.round(2),
                textposition='outside',
                textfont=dict(size=11),
                hovertemplate='%{x}<br>SÃ¼re: %{y:.2f}s<extra></extra>'
            ))
        
        if not s2_data.empty:
            time_values_s2 = pd.to_numeric(s2_data['Avg Time (s)'], errors='coerce')
            fig_time.add_trace(go.Bar(
                name='Scenario 2',
                x=s2_data['Model'],
                y=time_values_s2,
                marker_color=colors['secondary'],
                text=time_values_s2.round(2),
                textposition='outside',
                textfont=dict(size=11),
                hovertemplate='%{x}<br>SÃ¼re: %{y:.2f}s<extra></extra>'
            ))
        
        fig_time.update_layout(
            title=dict(text='YanÄ±t SÃ¼releri (saniye)', font=dict(size=16)),
            xaxis_title='',
            yaxis_title='SÃ¼re (s)',
            barmode='group',
            yaxis=dict(gridcolor='rgba(255,255,255,0.1)', rangemode='tozero'),
            xaxis=dict(gridcolor='rgba(255,255,255,0.1)'),
            legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),
            **layout_config
        )
        
        st.plotly_chart(fig_time, key="chart_time")
    
    # Ä°kinci satÄ±r: Semantic Skor ve RAM KullanÄ±mÄ±
    col3, col4 = st.columns(2)
    
    with col3:
        # Semantic Skor (tek senaryo iÃ§in daha iyi gÃ¶rÃ¼nÃ¼m)
        if not s1_data.empty:
            fig_semantic = go.Figure()
            
            # SayÄ±sal dÃ¶nÃ¼ÅŸÃ¼m
            semantic_values = pd.to_numeric(s1_data['Avg Semantic'], errors='coerce')
            
            # Renkleri skora gÃ¶re ayarla
            colors_list = [colors['success'] if x >= 80 else colors['primary'] if x >= 60 else colors['warning'] 
                          for x in semantic_values]
            
            fig_semantic.add_trace(go.Bar(
                x=s1_data['Model'],
                y=semantic_values,
                marker_color=colors_list,
                text=semantic_values.round(1),
                textposition='outside',
                textfont=dict(size=11),
                hovertemplate='%{x}<br>Semantic: %{y:.1f}<extra></extra>'
            ))
            
            fig_semantic.update_layout(
                title=dict(text='Anlamsal Benzerlik SkorlarÄ±', font=dict(size=16)),
                xaxis_title='',
                yaxis_title='Semantic Score',
                yaxis=dict(range=[0, 105], gridcolor='rgba(255,255,255,0.1)'),
                xaxis=dict(gridcolor='rgba(255,255,255,0.1)'),
                showlegend=False,
                **layout_config
            )
            
            st.plotly_chart(fig_semantic, key="chart_semantic")
    
    with col4:
        # Maliyet GrafiÄŸi (sadece API modelleri iÃ§in anlamlÄ±)
        cost_values = pd.to_numeric(summary_df['Total Cost ($)'], errors='coerce')
        api_data = summary_df[cost_values > 0].copy()
        
        if not api_data.empty:
            fig_cost = go.Figure()
            
            cost_vals = pd.to_numeric(api_data['Total Cost ($)'], errors='coerce')
            
            fig_cost.add_trace(go.Bar(
                x=api_data['Model'],
                y=cost_vals * 1000,  # Milicent olarak gÃ¶ster
                marker_color=colors['primary'],
                text=[f"${x:.4f}" for x in cost_vals],
                textposition='outside',
                textfont=dict(size=11),
                hovertemplate='%{x}<br>Maliyet: %{text}<extra></extra>'
            ))
            
            fig_cost.update_layout(
                title=dict(text='API Maliyetleri', font=dict(size=16)),
                xaxis_title='',
                yaxis_title='Maliyet (x1000 $)',
                yaxis=dict(gridcolor='rgba(255,255,255,0.1)', rangemode='tozero'),
                xaxis=dict(gridcolor='rgba(255,255,255,0.1)'),
                showlegend=False,
                **layout_config
            )
            
            st.plotly_chart(fig_cost, key="chart_cost")
        else:
            # RAM KullanÄ±mÄ± gÃ¶ster
            if 'Avg RAM (%)' in s1_data.columns:
                fig_ram = go.Figure()
                
                # SayÄ±sal dÃ¶nÃ¼ÅŸÃ¼m
                ram_values = pd.to_numeric(s1_data['Avg RAM (%)'], errors='coerce')
                
                fig_ram.add_trace(go.Bar(
                    x=s1_data['Model'],
                    y=ram_values,
                    marker_color=colors['secondary'],
                    text=ram_values.round(1),
                    textposition='outside',
                    textfont=dict(size=11),
                    hovertemplate='%{x}<br>RAM: %{y:.1f}%<extra></extra>'
                ))
                
                fig_ram.update_layout(
                    title=dict(text='Ortalama RAM KullanÄ±mÄ± (%)', font=dict(size=16)),
                    xaxis_title='',
                    yaxis_title='RAM (%)',
                    yaxis=dict(range=[0, 100], gridcolor='rgba(255,255,255,0.1)'),
                    xaxis=dict(gridcolor='rgba(255,255,255,0.1)'),
                    showlegend=False,
                    **layout_config
                )
                
                st.plotly_chart(fig_ram, key="chart_ram")


def display_radar_chart(summary_df: pd.DataFrame):
    """Model karÅŸÄ±laÅŸtÄ±rma radar grafiÄŸi."""
    if summary_df.empty:
        return
    
    # Sadece Scenario 1 verilerini kullan (adil karÅŸÄ±laÅŸtÄ±rma)
    s1_data = summary_df[summary_df['Scenario'] == 1].copy()
    
    if s1_data.empty:
        return
    
    st.subheader("Model KarÅŸÄ±laÅŸtÄ±rma Radar GrafiÄŸi")
    
    # Metrikleri normalize et (0-100 arasÄ±)
    metrics = ['Avg Score', 'Avg Semantic', 'Avg BERT', 'Avg ROUGE']
    
    # HÄ±z iÃ§in ters skor hesapla (dÃ¼ÅŸÃ¼k sÃ¼re = yÃ¼ksek skor)
    s1_data['Speed Score'] = 100 - (pd.to_numeric(s1_data['Avg Time (s)'], errors='coerce') / 
                                     pd.to_numeric(s1_data['Avg Time (s)'], errors='coerce').max() * 100)
    
    # Maliyet iÃ§in ters skor (dÃ¼ÅŸÃ¼k maliyet = yÃ¼ksek skor, local modeller 100)
    cost_values = pd.to_numeric(s1_data['Total Cost ($)'], errors='coerce')
    max_cost = cost_values.max() if cost_values.max() > 0 else 1
    s1_data['Cost Score'] = 100 - (cost_values / max_cost * 100)
    s1_data.loc[cost_values == 0, 'Cost Score'] = 100  # Local modeller bedava
    
    radar_metrics = ['Avg Score', 'Avg Semantic', 'Avg BERT', 'Speed Score', 'Cost Score']
    radar_labels = ['Genel Skor', 'Semantic', 'BERT', 'HÄ±z', 'Maliyet EtkinliÄŸi']
    
    fig = go.Figure()
    
    colors = ['#F5A623', '#4ECDC4', '#2ECC71', '#E74C3C', '#9B59B6', '#3498DB', '#1ABC9C']
    
    for idx, (_, row) in enumerate(s1_data.iterrows()):
        values = []
        for metric in radar_metrics:
            val = pd.to_numeric(row.get(metric, 0), errors='coerce')
            values.append(val if not pd.isna(val) else 0)
        
        # Radar iÃ§in deÄŸerleri kapat (ilk deÄŸeri sona ekle)
        values.append(values[0])
        labels = radar_labels + [radar_labels[0]]
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=labels,
            fill='toself',
            name=row['Model'],
            line_color=colors[idx % len(colors)],
            opacity=0.7
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                tickfont=dict(size=10)
            ),
            bgcolor='rgba(0,0,0,0)'
        ),
        showlegend=True,
        legend=dict(
            orientation='h',
            yanchor='bottom',
            y=-0.2,
            xanchor='center',
            x=0.5
        ),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(color='#FAFAFA'),
        height=500,
        margin=dict(t=30, b=80)
    )
    
    st.plotly_chart(fig, key="radar_chart", use_container_width=True)
    
    # Radar aÃ§Ä±klamasÄ±
    with st.expander("â„¹ï¸ Radar grafiÄŸi nasÄ±l okunur?", expanded=False):
        st.markdown("""
        **Her eksen bir performans metriÄŸini temsil eder:**
        
        | Eksen | AÃ§Ä±klama |
        |-------|----------|
        | **Genel Skor** | AÄŸÄ±rlÄ±klÄ± ortalama (Semantic %60 + BERT %30 + ROUGE %10) |
        | **Semantic** | Anlamsal benzerlik skoru |
        | **BERT** | BERTScore F1 deÄŸeri |
        | **HÄ±z** | YanÄ±t sÃ¼resi (dÃ¼ÅŸÃ¼k sÃ¼re = yÃ¼ksek skor) |
        | **Maliyet EtkinliÄŸi** | DÃ¼ÅŸÃ¼k maliyet = yÃ¼ksek skor (Local modeller 100) |
        
        **Daha geniÅŸ alan = daha iyi performans**
        """)


def generate_ai_analysis(summary_df: pd.DataFrame, anthropic_api_key: str) -> str:
    """Claude API ile benchmark sonuÃ§larÄ±nÄ± yorumla."""
    if not anthropic_api_key:
        return None
    
    try:
        from langchain_anthropic import ChatAnthropic
        
        # Sadece Scenario 1 verilerini kullan
        s1_data = summary_df[summary_df['Scenario'] == 1].copy()
        
        if s1_data.empty:
            return None
        
        # En iyi ve en kÃ¶tÃ¼ modeli bul
        s1_data['Avg Score Num'] = pd.to_numeric(s1_data['Avg Score'], errors='coerce')
        best_model = s1_data.loc[s1_data['Avg Score Num'].idxmax()]
        worst_model = s1_data.loc[s1_data['Avg Score Num'].idxmin()]
        
        # API ve Local ayÄ±rÄ±mÄ±
        api_models = s1_data[s1_data['Total Cost ($)'].apply(lambda x: pd.to_numeric(x, errors='coerce') > 0)]
        local_models = s1_data[s1_data['Total Cost ($)'].apply(lambda x: pd.to_numeric(x, errors='coerce') == 0)]
        
        # Prompt oluÅŸtur
        prompt = f"""Sen bir RAG (Retrieval-Augmented Generation) sistemi danÄ±ÅŸmanÄ±sÄ±n. AÅŸaÄŸÄ±daki benchmark sonuÃ§larÄ±nÄ± analiz et ve kullanÄ±cÄ±ya hangi modeli seÃ§mesi gerektiÄŸi konusunda profesyonel bir Ã¶neri sun.

## Benchmark SonuÃ§larÄ± (Scenario 1 - Adil KarÅŸÄ±laÅŸtÄ±rma)

{s1_data[['Model', 'Avg Score', 'Avg Semantic', 'Avg BERT', 'Avg Time (s)', 'Total Cost ($)']].to_string(index=False)}

## Analiz Ä°stekleri:

1. **En Ä°yi Model:** {best_model['Model']} ({best_model['Avg Score']:.1f} puan)
2. **Dikkat Edilmesi Gereken Model:** {worst_model['Model']} ({worst_model['Avg Score']:.1f} puan)

LÃ¼tfen ÅŸu baÅŸlÄ±klar altÄ±nda kÄ±sa ve Ã¶z bir analiz yap (TÃ¼rkÃ§e):

### ðŸ† Genel DeÄŸerlendirme
(Hangi model en iyi performansÄ± gÃ¶sterdi ve neden?)

### ðŸ’° Maliyet-Performans Analizi  
(API vs Local modeller karÅŸÄ±laÅŸtÄ±rmasÄ±)

### ðŸŽ¯ KullanÄ±m Senaryosu Ã–nerileri
(Hangi durumlarda hangi model tercih edilmeli?)

### âš¡ SonuÃ§ ve Tavsiye
(Tek cÃ¼mlelik net Ã¶neri)

Not: YanÄ±tÄ±n 300 kelimeyi geÃ§mesin. Markdown formatÄ±nda yaz."""

        # Claude'a gÃ¶nder
        llm = ChatAnthropic(
            model="claude-3-5-haiku-latest",
            api_key=anthropic_api_key,
            temperature=0.3,
            max_tokens=1000
        )
        
        response = llm.invoke(prompt)
        return response.content
        
    except Exception as e:
        print(f"AI analiz hatasÄ±: {e}")
        return None


def main():
    """Ana Streamlit uygulamasÄ±."""
    
    # Sayfa yapÄ±landÄ±rmasÄ±
    st.set_page_config(
        page_title="RAG Benchmark",
        page_icon="â—†",
        layout="wide"
    )
    
    # Session state
    init_session_state()
    
    # BaÅŸlÄ±k
    st.title("Multi-Model RAG Benchmark")
    st.markdown("""
    API Modelleri (Gemini, GPT, Claude) ve Local Modelleri (Ollama: Llama, Mistral, Phi, Qwen) 
    iki farklÄ± senaryoda karÅŸÄ±laÅŸtÄ±rÄ±n.
    
    **Senaryo 1 â€” Fair Arena:** TÃ¼m modeller aynÄ± embedding ile test edilir  
    **Senaryo 2 â€” Real World:** Local modeller kendi optimize edilmiÅŸ embedding'leri ile test edilir
    """)
    
    st.divider()
    
    # Sistem durumu
    st.subheader("Sistem Durumu")
    display_system_status()
    
    # API keylerini .env'den oku (UI'da gÃ¶sterme)
    gemini_api_key = os.getenv("GOOGLE_API_KEY", "")
    openai_api_key = os.getenv("OPENAI_API_KEY", "")
    anthropic_api_key = os.getenv("ANTHROPIC_API_KEY", "")
    
    # Model seÃ§imi
    st.sidebar.header("Model SeÃ§imi")
    st.sidebar.caption("API Modelleri")
    
    use_gemini = st.sidebar.checkbox(
        "Gemini 2.5 Flash", value=bool(gemini_api_key),
        help="Google'Ä±n Gemini 2.5 Flash modeli. HÄ±zlÄ± ve maliyet etkin.",
        disabled=not bool(gemini_api_key)
    )
    use_gpt = st.sidebar.checkbox(
        "GPT-3.5 Turbo", value=False,
        help="OpenAI'Ä±n GPT-3.5 Turbo modeli. GÃ¼venilir ve yaygÄ±n kullanÄ±lan.",
        disabled=not bool(openai_api_key)
    )
    use_claude = st.sidebar.checkbox(
        "Claude 3.5 Haiku", value=bool(anthropic_api_key),
        help="Anthropic'in en hÄ±zlÄ± ve ucuz Claude modeli.",
        disabled=not bool(anthropic_api_key)
    )
    
    st.sidebar.divider()
    st.sidebar.caption("ðŸ¦™ Ollama Modelleri (Local)")
    
    use_llama = st.sidebar.checkbox(
        "Llama 3.1 (8B)", value=True,
        help="Meta'nÄ±n gÃ¼Ã§lÃ¼ aÃ§Ä±k kaynak modeli. Genel amaÃ§lÄ±, dengeli performans."
    )
    use_mistral = st.sidebar.checkbox(
        "Mistral (7B)", value=False,
        help="HÄ±zlÄ± ve verimli FransÄ±z modeli. Kod ve mantÄ±k iÅŸlemlerinde gÃ¼Ã§lÃ¼."
    )
    use_phi = st.sidebar.checkbox(
        "Phi-3 (3.8B)", value=False,
        help="Microsoft'un kÃ¼Ã§Ã¼k ama gÃ¼Ã§lÃ¼ modeli. DÃ¼ÅŸÃ¼k RAM kullanÄ±mÄ±."
    )
    use_qwen = st.sidebar.checkbox(
        "Qwen 2.5 (7B)", value=False,
        help="Alibaba'nÄ±n Ã§ok dilli modeli. Matematik ve kodlamada baÅŸarÄ±lÄ±."
    )
    
    # Senaryo seÃ§imi
    st.sidebar.header("Senaryo SeÃ§imi")
    
    with st.sidebar.expander("â„¹ï¸ Senaryolar hakkÄ±nda", expanded=False):
        st.markdown("""
        **Scenario 1 (Shared â€” Fair Arena):**
        - TÃ¼m modeller aynÄ± embedding modeli kullanÄ±r (MiniLM-L6-v2)
        - Adil karÅŸÄ±laÅŸtÄ±rma: AynÄ± context ile sadece LLM yetenekleri test edilir
        - API ve Local modeller eÅŸit ÅŸartlarda yarÄ±ÅŸÄ±r
        
        **Scenario 2 (Own â€” Real World):**
        - Her local model kendi optimize embedding'ini kullanÄ±r
        - Llama â†’ BGE-Large, Mistral â†’ MPNet, Phi â†’ Multilingual, Qwen â†’ BGE-Base
        - GerÃ§ek dÃ¼nya performansÄ±nÄ± simÃ¼le eder
        - Sadece local modeller test edilir
        """)
    
    run_scenario_1 = st.sidebar.checkbox(
        "Scenario 1 (Shared)", 
        value=True,
        help="TÃ¼m modeller aynÄ± MiniLM-L6-v2 embedding ile test edilir"
    )
    run_scenario_2 = st.sidebar.checkbox(
        "Scenario 2 (Own)", 
        value=False,
        help="Her local model kendi embedding modeli ile test edilir"
    )
    
    # Ayarlar
    st.sidebar.header("âš™ Ayarlar")
    
    with st.sidebar.expander("â„¹ï¸ Parametreler hakkÄ±nda", expanded=False):
        st.markdown("""
        **Chunk Size:** Corpus'un kaÃ§ karakterlik parÃ§alara bÃ¶lÃ¼neceÄŸi
        - KÄ±sa metinler â†’ 500-800
        - Uzun metinler â†’ 1000-1500
        - Paragraf bazlÄ± â†’ 1500-2000
        
        **Chunk Overlap:** ParÃ§alar arasÄ± Ã¶rtÃ¼ÅŸme miktarÄ±
        - BaÄŸlam kaybÄ±nÄ± Ã¶nler
        - Genelde Chunk Size'Ä±n %10-20'si
        
        **Retriever K:** Her soru iÃ§in kaÃ§ dokÃ¼man getirilecek
        - K=1-2 â†’ HÄ±zlÄ±, dar odak
        - K=3-5 â†’ Dengeli (Ã¶nerilen)
        - K=6+ â†’ GeniÅŸ context, yavaÅŸ
        """)
    
    chunk_size = st.sidebar.slider(
        "Chunk Size", 500, 2000, 1000, 100,
        help="Corpus'un kaÃ§ karakterlik parÃ§alara bÃ¶lÃ¼neceÄŸi. KÄ±sa metinler iÃ§in dÃ¼ÅŸÃ¼k, uzun metinler iÃ§in yÃ¼ksek deÄŸer seÃ§in."
    )
    chunk_overlap = st.sidebar.slider(
        "Chunk Overlap", 0, 500, 200, 50,
        help="ParÃ§alar arasÄ± Ã¶rtÃ¼ÅŸme. BaÄŸlam kaybÄ±nÄ± Ã¶nler. Chunk Size'Ä±n %10-20'si Ã¶nerilir."
    )
    retriever_k = st.sidebar.slider(
        "Retriever K", 1, 10, 3, 1,
        help="Her soru iÃ§in kaÃ§ dokÃ¼man getirilecek. 3-5 arasÄ± dengeli sonuÃ§ verir."
    )
    
    st.sidebar.divider()
    st.sidebar.header("ðŸ“Š Test AyarlarÄ±")
    
    max_questions = st.sidebar.slider(
        "Test Soru SayÄ±sÄ±", 8, 64, 16, 8,
        help="Benchmark iÃ§in kullanÄ±lacak soru sayÄ±sÄ±. Daha fazla soru = daha gÃ¼venilir sonuÃ§, ama daha uzun sÃ¼re."
    )
    
    st.divider()
    
    # Dosya yÃ¼kleyiciler
    st.subheader("Dosya YÃ¼kleme")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Corpus CSV**")
        corpus_file = st.file_uploader(
            "KÃ¼tÃ¼phane/Corpus CSV dosyasÄ±nÄ± yÃ¼kleyin",
            type=["csv"],
            key="corpus",
            label_visibility="collapsed"
        )
        if corpus_file:
            st.success(f"{corpus_file.name} yÃ¼klendi")
    
    with col2:
        st.markdown("**Test CSV**")
        test_file = st.file_uploader(
            "Test sorularÄ± CSV dosyasÄ±nÄ± yÃ¼kleyin",
            type=["csv"],
            key="test",
            label_visibility="collapsed"
        )
        if test_file:
            st.success(f"{test_file.name} yÃ¼klendi")
    
    # Ollama durumu
    st.subheader("Ollama Durumu")
    
    model_manager = ModelManager()
    display_ollama_status(model_manager)
    
    st.divider()
    
    # Test butonu
    if st.button("Benchmark BaÅŸlat", type="primary", use_container_width=True):
        # Validasyonlar
        if not corpus_file:
            st.error("LÃ¼tfen corpus CSV dosyasÄ± yÃ¼kleyin.")
            return
        
        if not test_file:
            st.error("LÃ¼tfen test CSV dosyasÄ± yÃ¼kleyin.")
            return
        
        if not run_scenario_1 and not run_scenario_2:
            st.error("En az bir senaryo seÃ§in.")
            return
        
        # SeÃ§ilen modeller
        selected_ollama = []
        if use_llama:
            selected_ollama.append("Llama")
        if use_mistral:
            selected_ollama.append("Mistral")
        if use_phi:
            selected_ollama.append("Phi")
        if use_qwen:
            selected_ollama.append("Qwen")
        
        if not use_gemini and not use_gpt and not use_claude and not selected_ollama:
            st.error("En az bir model seÃ§in.")
            return
        
        # Benchmark Ã§alÄ±ÅŸtÄ±r
        try:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # CSV Ä°ÅŸleme
            status_text.text("Corpus iÅŸleniyor...")
            csv_processor = CSVProcessor(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            documents = csv_processor.load_and_chunk(corpus_file)
            st.info(f"{len(documents)} chunk oluÅŸturuldu")
            progress_bar.progress(10)
            
            # Test CSV
            status_text.text("Test dosyasÄ± yÃ¼kleniyor...")
            test_df_full = csv_processor.load_test_csv(test_file)
            
            # KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi soru sayÄ±sÄ± kadar rastgele seÃ§
            if len(test_df_full) > max_questions:
                test_df = test_df_full.sample(n=max_questions, random_state=None).reset_index(drop=True)
                st.info(f"{len(test_df_full)} sorudan rastgele {max_questions} tanesi seÃ§ildi")
            else:
                test_df = test_df_full
                st.info(f"{len(test_df)} test sorusu yÃ¼klendi")
            progress_bar.progress(15)
            
            # RAG Pipeline
            status_text.text("RAG Pipeline kuruluyor...")
            rag_pipeline = RAGPipeline(documents)
            
            # Shared embedding (Scenario 1 iÃ§in)
            if run_scenario_1:
                rag_pipeline.setup_shared_embedding()
            progress_bar.progress(25)
            
            # Model Manager
            status_text.text("Modeller yÃ¼kleniyor...")
            model_manager.setup_models(
                gemini_api_key=gemini_api_key if use_gemini else None,
                openai_api_key=openai_api_key if use_gpt else None,
                anthropic_api_key=anthropic_api_key if use_claude else None,
                use_ollama=bool(selected_ollama),
                ollama_models=selected_ollama if selected_ollama else None
            )
            st.info(f"{len(model_manager.get_available_models())} model yÃ¼klendi")
            progress_bar.progress(35)
            
            # Evaluator
            status_text.text("DeÄŸerlendirme modÃ¼lleri hazÄ±rlanÄ±yor...")
            evaluator = UnifiedEvaluator()
            progress_bar.progress(45)
            
            # Benchmark Runner
            benchmark_runner = BenchmarkRunner(
                rag_pipeline=rag_pipeline,
                model_manager=model_manager,
                evaluator=evaluator,
                hw_monitor=st.session_state.hw_monitor
            )
            
            # SenaryolarÄ± belirle
            scenarios = []
            if run_scenario_1:
                scenarios.append(1)
            if run_scenario_2:
                scenarios.append(2)
            
            # Progress callback
            def update_progress(pct):
                progress_bar.progress(int(45 + pct * 50))
            
            # Benchmark Ã§alÄ±ÅŸtÄ±r
            status_text.text("Benchmark Ã§alÄ±ÅŸÄ±yor...")
            
            results = benchmark_runner.run_full_benchmark(
                test_df=test_df,
                scenarios=scenarios,
                k=retriever_k,
                progress_callback=update_progress
            )
            
            progress_bar.progress(95)
            
            # Ã–zet oluÅŸtur
            status_text.text("SonuÃ§lar hazÄ±rlanÄ±yor...")
            summary_df = benchmark_runner.generate_summary(results)
            comparison_df = benchmark_runner.generate_comparison(results)
            
            progress_bar.progress(100)
            status_text.text("Benchmark tamamlandÄ±.")
            
            # SonuÃ§larÄ± session state'e kaydet
            st.session_state.benchmark_results = {
                'results': results,
                'summary': summary_df,
                'comparison': comparison_df
            }
            st.session_state.test_completed = True
            
            st.success("Benchmark baÅŸarÄ±yla tamamlandÄ±.")
            
        except Exception as e:
            st.error(f"Hata oluÅŸtu: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
    
    # SonuÃ§larÄ± gÃ¶ster
    if st.session_state.test_completed and st.session_state.benchmark_results:
        st.divider()
        st.header("Benchmark SonuÃ§larÄ±")
        
        results = st.session_state.benchmark_results
        
        # Ã–zet tablo
        st.subheader("Model Performans Ã–zeti")
        display_results_table(results['summary'])
        
        # KarÅŸÄ±laÅŸtÄ±rma
        if not results['comparison'].empty:
            display_comparison_table(results['comparison'])
        
        # Grafikler
        display_charts(results['summary'])
        
        # Radar Chart
        display_radar_chart(results['summary'])
        
        # AI Analizi (Claude ile)
        if anthropic_api_key:
            st.subheader("ðŸ¤– AI Destekli Analiz")
            
            with st.spinner("Claude analiz yapÄ±yor..."):
                ai_analysis = generate_ai_analysis(results['summary'], anthropic_api_key)
                
            if ai_analysis:
                st.markdown(ai_analysis)
            else:
                st.info("AI analizi oluÅŸturulamadÄ±.")
        
        # Ham veriler
        with st.expander("Ham Veriler"):
            if 'scenario_1' in results['results']:
                st.markdown("**Scenario 1 DetaylarÄ±**")
                s1_data = [r.to_dict() for r in results['results']['scenario_1']]
                st.dataframe(pd.DataFrame(s1_data))
            
            if 'scenario_2' in results['results']:
                st.markdown("**Scenario 2 DetaylarÄ±**")
                s2_data = [r.to_dict() for r in results['results']['scenario_2']]
                st.dataframe(pd.DataFrame(s2_data))
        
        # CSV Ä°ndirme
        st.subheader("SonuÃ§larÄ± Ä°ndir")
        
        col1, col2 = st.columns(2)
        
        with col1:
            summary_csv = results['summary'].to_csv(index=False)
            st.download_button(
                "Ã–zet Tabloyu Ä°ndir (CSV)",
                summary_csv,
                "benchmark_summary.csv",
                "text/csv"
            )
        
        with col2:
            if not results['comparison'].empty:
                comparison_csv = results['comparison'].to_csv(index=False)
                st.download_button(
                    "KarÅŸÄ±laÅŸtÄ±rmayÄ± Ä°ndir (CSV)",
                    comparison_csv,
                    "benchmark_comparison.csv",
                    "text/csv"
                )
    
    # Footer
    st.markdown("---")
    st.caption("Enhanced RAG Benchmark System v2.0")


if __name__ == "__main__":
    main()
