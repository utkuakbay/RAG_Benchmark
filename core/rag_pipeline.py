"""
RAG Pipeline - FAISS vektÃ¶r veritabanÄ± ve retriever yÃ¶netimi.

Ä°ki senaryo destekler:
1. Shared Embedding: TÃ¼m modeller aynÄ± embedding'i kullanÄ±r
2. Model-Specific Embedding: Her model kendi embedding'ini kullanÄ±r

Ã–zellikler:
- GPU otomatik algÄ±lama (CUDA varsa kullanÄ±r)
- Vectorstore cache (diske kaydetme/yÃ¼kleme)
"""

import time
import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path

import torch
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Config import
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.model_config import (
    SHARED_EMBEDDING, 
    EMBEDDING_MODELS, 
    MODEL_CONFIG,
    get_model_embedding
)

# Cache dizini
CACHE_DIR = Path(__file__).parent.parent / "cache" / "vectorstores"


class RAGPipeline:
    """
    RAG Pipeline - FAISS vektÃ¶r veritabanÄ± ve retriever yÃ¶netimi.
    
    Ä°ki senaryo:
    1. Shared: TÃ¼m modeller aynÄ± embedding'i kullanÄ±r
    2. Model-Specific: Her local model kendi embedding'ini kullanÄ±r
    
    Ã–zellikler:
    - GPU otomatik algÄ±lama
    - Vectorstore cache (disk'e kaydet/yÃ¼kle)
    """
    
    def __init__(self, documents: List[Document], use_cache: bool = True):
        """
        Args:
            documents: Chunk'lanmÄ±ÅŸ dokÃ¼man listesi
            use_cache: Vectorstore cache kullanÄ±lsÄ±n mÄ±
        """
        self.documents = documents
        self.use_cache = use_cache
        
        # GPU algÄ±lama
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Shared embedding ve vectorstore
        self.shared_embedding = None
        self.shared_vectorstore = None
        
        # Model-specific embedding'ler ve vectorstore'lar
        self.model_embeddings: Dict[str, Any] = {}
        self.model_vectorstores: Dict[str, Any] = {}
        
        # DokÃ¼man hash'i (cache iÃ§in)
        self._doc_hash = self._calculate_doc_hash()
        
        # Cache dizinini oluÅŸtur
        if use_cache:
            CACHE_DIR.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ“š RAGPipeline baÅŸlatÄ±ldÄ±: {len(documents)} dokÃ¼man")
        print(f"   Device: {self.device.upper()}")
        print(f"   Cache: {'Aktif' if use_cache else 'KapalÄ±'}")
    
    def _calculate_doc_hash(self) -> str:
        """DokÃ¼man iÃ§eriÄŸinden hash hesapla (cache key iÃ§in)."""
        content = "".join([doc.page_content[:100] for doc in self.documents[:10]])
        content += str(len(self.documents))
        return hashlib.md5(content.encode()).hexdigest()[:8]
    
    def _get_cache_path(self, embedding_name: str) -> Path:
        """Cache dosya yolunu dÃ¶ndÃ¼r."""
        safe_name = embedding_name.replace("/", "_").replace(":", "_")
        return CACHE_DIR / f"{safe_name}_{self._doc_hash}"
    
    def _load_from_cache(self, embedding_name: str, embedding) -> Optional[FAISS]:
        """Cache'den vectorstore yÃ¼kle."""
        if not self.use_cache:
            return None
        
        cache_path = self._get_cache_path(embedding_name)
        if cache_path.exists():
            try:
                vectorstore = FAISS.load_local(
                    str(cache_path), 
                    embedding,
                    allow_dangerous_deserialization=True
                )
                print(f"   âœ… Cache'den yÃ¼klendi: {cache_path.name}")
                return vectorstore
            except Exception as e:
                print(f"   âš ï¸ Cache yÃ¼klenemedi: {e}")
        return None
    
    def _save_to_cache(self, embedding_name: str, vectorstore: FAISS) -> None:
        """Vectorstore'u cache'e kaydet."""
        if not self.use_cache:
            return
        
        cache_path = self._get_cache_path(embedding_name)
        try:
            vectorstore.save_local(str(cache_path))
            print(f"   ğŸ’¾ Cache'e kaydedildi: {cache_path.name}")
        except Exception as e:
            print(f"   âš ï¸ Cache kaydedilemedi: {e}")
    
    def setup_shared_embedding(self, embedding_model: str = None) -> None:
        """
        Scenario 1: Shared embedding kurulumu.
        
        TÃ¼m modeller bu embedding'i kullanacak.
        
        Args:
            embedding_model: KullanÄ±lacak embedding modeli (None = default)
        """
        model_name = embedding_model or SHARED_EMBEDDING
        
        print(f"\n{'='*60}")
        print(f"SHARED EMBEDDING OLUÅTURULUYOR")
        print(f"{'='*60}")
        print(f"Model: {model_name}")
        print(f"Device: {self.device.upper()}")
        
        try:
            start_time = time.time()
            
            # Embedding modeli oluÅŸtur (GPU destekli)
            self.shared_embedding = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # Ã–nce cache'den yÃ¼klemeyi dene
            cached = self._load_from_cache(model_name, self.shared_embedding)
            if cached:
                self.shared_vectorstore = cached
                elapsed_time = time.time() - start_time
                print(f"âœ… Shared vectorstore hazÄ±r ({elapsed_time:.2f} saniye)")
                print(f"   Ä°ndekslenen dokÃ¼man: {len(self.documents)}")
                print(f"{'='*60}\n")
                return
            
            # Cache yoksa yeni oluÅŸtur
            print("   ğŸ“Š Yeni vectorstore oluÅŸturuluyor...")
            self.shared_vectorstore = FAISS.from_documents(
                documents=self.documents,
                embedding=self.shared_embedding
            )
            
            # Cache'e kaydet
            self._save_to_cache(model_name, self.shared_vectorstore)
            
            elapsed_time = time.time() - start_time
            
            print(f"âœ… Shared vectorstore oluÅŸturuldu ({elapsed_time:.2f} saniye)")
            print(f"   Ä°ndekslenen dokÃ¼man: {len(self.documents)}")
            print(f"{'='*60}\n")
            
        except Exception as e:
            raise Exception(f"Shared embedding oluÅŸturma hatasÄ±: {str(e)}")
    
    def setup_model_specific_embedding(self, model_name: str) -> None:
        """
        Scenario 2: Model-specific embedding kurulumu.
        
        Args:
            model_name: Model adÄ± (Llama, Mistral, Phi, Qwen)
        """
        # Model iÃ§in embedding al
        embedding_model = get_model_embedding(model_name, scenario=2)
        
        if embedding_model is None:
            print(f"âš ï¸ {model_name} iÃ§in Scenario 2 embedding tanÄ±mlÄ± deÄŸil")
            return
        
        # Zaten oluÅŸturulmuÅŸ mu kontrol et
        if model_name in self.model_vectorstores:
            print(f"âœ… {model_name} vectorstore zaten mevcut (bellekte)")
            return
        
        print(f"\nğŸ“Š {model_name} iÃ§in Ã¶zel embedding oluÅŸturuluyor...")
        print(f"   Model: {embedding_model}")
        print(f"   Device: {self.device.upper()}")
        
        try:
            start_time = time.time()
            
            # Embedding modeli oluÅŸtur (GPU destekli)
            embedding = HuggingFaceEmbeddings(
                model_name=embedding_model,
                model_kwargs={'device': self.device},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # Ã–nce cache'den yÃ¼klemeyi dene
            cached = self._load_from_cache(embedding_model, embedding)
            if cached:
                self.model_embeddings[model_name] = embedding
                self.model_vectorstores[model_name] = cached
                elapsed_time = time.time() - start_time
                print(f"   âœ… {model_name} vectorstore hazÄ±r ({elapsed_time:.2f} saniye)")
                return
            
            # Cache yoksa yeni oluÅŸtur
            print("   ğŸ“Š Yeni vectorstore oluÅŸturuluyor...")
            vectorstore = FAISS.from_documents(
                documents=self.documents,
                embedding=embedding
            )
            
            # Cache'e kaydet
            self._save_to_cache(embedding_model, vectorstore)
            
            elapsed_time = time.time() - start_time
            
            # Kaydet
            self.model_embeddings[model_name] = embedding
            self.model_vectorstores[model_name] = vectorstore
            
            print(f"   âœ… {model_name} vectorstore hazÄ±r ({elapsed_time:.2f} saniye)")
            
        except Exception as e:
            print(f"   âŒ {model_name} embedding hatasÄ±: {str(e)}")
    
    def setup_all_model_specific_embeddings(self, model_names: List[str] = None) -> None:
        """
        TÃ¼m local modeller iÃ§in model-specific embedding'leri oluÅŸtur.
        
        Args:
            model_names: Model listesi (None = tÃ¼m local modeller)
        """
        if model_names is None:
            # Sadece local modelleri al
            model_names = [
                name for name, config in MODEL_CONFIG.items()
                if config.get("type") == "local"
            ]
        
        print(f"\n{'='*60}")
        print(f"MODEL-SPECIFIC EMBEDDINGS OLUÅTURULUYOR")
        print(f"{'='*60}")
        print(f"Modeller: {', '.join(model_names)}")
        
        for model_name in model_names:
            self.setup_model_specific_embedding(model_name)
        
        print(f"{'='*60}\n")
    
    def get_retriever(
        self, 
        model_name: str = None, 
        scenario: int = 1, 
        k: int = 3
    ):
        """
        Retriever objesi dÃ¶ndÃ¼r.
        
        Args:
            model_name: Model adÄ± (Scenario 2 iÃ§in gerekli)
            scenario: 1 = Shared, 2 = Model-specific
            k: En alakalÄ± k dokÃ¼manÄ± getir
            
        Returns:
            Retriever objesi
        """
        if scenario == 1:
            # Shared embedding
            if self.shared_vectorstore is None:
                raise Exception("Shared vectorstore henÃ¼z oluÅŸturulmamÄ±ÅŸ!")
            
            return self.shared_vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
        
        elif scenario == 2:
            # Model-specific embedding
            if model_name is None:
                raise Exception("Scenario 2 iÃ§in model_name gerekli!")
            
            if model_name not in self.model_vectorstores:
                # Otomatik oluÅŸtur
                self.setup_model_specific_embedding(model_name)
            
            if model_name not in self.model_vectorstores:
                raise Exception(f"{model_name} iÃ§in vectorstore oluÅŸturulamadÄ±!")
            
            return self.model_vectorstores[model_name].as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
        
        else:
            raise Exception(f"GeÃ§ersiz scenario: {scenario}")
    
    def retrieve(
        self,
        query: str,
        model_name: str = None,
        scenario: int = 1,
        k: int = 3
    ) -> tuple:
        """
        Sorgu iÃ§in dokÃ¼manlarÄ± retrieve et.
        
        Args:
            query: Sorgu metni
            model_name: Model adÄ±
            scenario: 1 = Shared, 2 = Model-specific
            k: En alakalÄ± k dokÃ¼manÄ± getir
            
        Returns:
            (documents, retrieval_time)
        """
        retriever = self.get_retriever(model_name, scenario, k)
        
        start_time = time.time()
        docs = retriever.invoke(query)
        retrieval_time = time.time() - start_time
        
        return docs, retrieval_time
    
    def get_context(self, docs: List[Document]) -> str:
        """DokÃ¼manlardan context metni oluÅŸtur."""
        return "\n\n".join([doc.page_content for doc in docs])
    
    def get_doc_ids(self, docs: List[Document]) -> List[int]:
        """DokÃ¼manlarÄ±n row_index'lerini dÃ¶ndÃ¼r."""
        return [doc.metadata.get("row_index", -1) for doc in docs]
    
    def get_available_scenarios(self, model_name: str) -> List[int]:
        """
        Model iÃ§in kullanÄ±labilir senaryolarÄ± dÃ¶ndÃ¼r.
        
        Args:
            model_name: Model adÄ±
            
        Returns:
            KullanÄ±labilir senaryo numaralarÄ± listesi
        """
        scenarios = []
        
        # Scenario 1 her zaman kullanÄ±labilir (shared varsa)
        if self.shared_vectorstore is not None:
            scenarios.append(1)
        
        # Scenario 2 sadece local modeller iÃ§in
        config = MODEL_CONFIG.get(model_name, {})
        if config.get("type") == "local" and config.get("scenario_2_embedding"):
            scenarios.append(2)
        
        return scenarios
