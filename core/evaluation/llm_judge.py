"""
LLM Judge - Cross-validation i√ßin LLM tabanlƒ± deƒüerlendirme.

Primary metriklerin g√ºvenilirliƒüini doƒürulamak i√ßin %20 sample deƒüerlendirir.
"""

import random
from typing import Dict, Any, List, Optional

try:
    from scipy.stats import pearsonr
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("‚ö†Ô∏è scipy y√ºkl√º deƒüil. Correlation analizi devre dƒ±≈üƒ±.")


class LLMJudge:
    """
    LLM-as-a-Judge - Cross-validation i√ßin.
    
    T√ºm sorularƒ± deƒüil, sadece random %20'sini deƒüerlendirerek
    primary metriklerin g√ºvenilirliƒüini test eder.
    """
    
    # Deƒüerlendirme prompt'u
    JUDGE_PROMPT = """Bir deƒüerlendirici olarak, verilen soruya modelin cevabƒ±nƒ±n kalitesini puanla.

SORU: {question}

ƒ∞DEAL CEVAP: {ideal_answer}

MODEL CEVABI: {model_answer}

A≈üaƒüƒ±daki kriterlere g√∂re 0-100 arasƒ± puan ver:
1. Doƒüruluk (40%): Cevap fakt√ºel olarak doƒüru mu?
2. Tamlƒ±k (30%): ƒ∞deal cevaptaki t√ºm bilgiler var mƒ±?
3. Alakalƒ±lƒ±k (20%): Cevap soruyla alakalƒ± mƒ±?
4. Netlik (10%): Cevap a√ßƒ±k ve anla≈üƒ±lƒ±r mƒ±?

SADECE bir sayƒ± d√∂nd√ºr (0-100 arasƒ±). A√ßƒ±klama yapma.
PUAN:"""

    def __init__(
        self, 
        llm: Optional[Any] = None,
        sample_rate: float = 0.20
    ):
        """
        Args:
            llm: Deƒüerlendirme i√ßin kullanƒ±lacak LLM (None ise devre dƒ±≈üƒ±)
            sample_rate: Deƒüerlendirilecek oran (default: %20)
        """
        self.llm = llm
        self.sample_rate = sample_rate
        self.available = llm is not None
        
        if self.available:
            print(f"‚úÖ LLMJudge hazƒ±r (sample rate: %{sample_rate*100:.0f})")
        else:
            print("‚ö†Ô∏è LLMJudge: LLM saƒülanmadƒ±, devre dƒ±≈üƒ±")
    
    def set_llm(self, llm: Any) -> None:
        """LLM'i sonradan ayarla."""
        self.llm = llm
        self.available = llm is not None
        if self.available:
            print(f"‚úÖ LLMJudge LLM ayarlandƒ±")
    
    def evaluate_single(
        self,
        question: str,
        ideal_answer: str,
        model_answer: str
    ) -> Dict[str, Any]:
        """
        Tek bir cevabƒ± deƒüerlendir.
        
        Args:
            question: Soru
            ideal_answer: ƒ∞deal cevap
            model_answer: Model cevabƒ±
            
        Returns:
            dict: score ve detaylar
        """
        if not self.available:
            return {"score": 0.0, "error": "LLM not available"}
        
        try:
            # Prompt olu≈ütur
            prompt = self.JUDGE_PROMPT.format(
                question=question,
                ideal_answer=ideal_answer,
                model_answer=model_answer
            )
            
            # LLM'e sor
            response = self.llm.invoke(prompt)
            
            # Response'dan sayƒ±yƒ± √ßƒ±kar
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # Sayƒ±yƒ± bul
            import re
            numbers = re.findall(r'\d+(?:\.\d+)?', response_text)
            
            if numbers:
                score = float(numbers[0])
                score = max(0.0, min(100.0, score))  # 0-100 sƒ±nƒ±rla
            else:
                score = 0.0
            
            return {
                "score": score,
                "raw_response": response_text
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è LLM Judge deƒüerlendirme hatasƒ±: {e}")
            return {"score": 0.0, "error": str(e)}
    
    def cross_validate(
        self, 
        results: List[Dict[str, Any]],
        sample_rate: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Primary metrikleri doƒürulamak i√ßin sample-based deƒüerlendirme.
        
        Args:
            results: T√ºm test sonu√ßlarƒ±
                - Her biri: {question, ideal_answer, answer, final_score, ...}
            sample_rate: Deƒüerlendirilecek oran (None ise self.sample_rate)
            
        Returns:
            dict: Korelasyon analizi sonu√ßlarƒ±
        """
        if not self.available:
            return {
                "error": "LLM not available",
                "correlation": 0.0,
                "sample_size": 0
            }
        
        if not results:
            return {
                "error": "No results to validate",
                "correlation": 0.0,
                "sample_size": 0
            }
        
        rate = sample_rate if sample_rate is not None else self.sample_rate
        
        # Random sample se√ß
        total = len(results)
        sample_size = max(2, int(total * rate))  # En az 2 sample
        sample_size = min(sample_size, total)  # Total'dan fazla olamaz
        
        sample_indices = random.sample(range(total), sample_size)
        
        print(f"\nüéØ LLM Judge Cross-Validation")
        print(f"   Total: {total} soru")
        print(f"   Sample: {sample_size} soru (%{rate*100:.0f})")
        
        correlations = []
        
        for idx in sample_indices:
            result = results[idx]
            
            try:
                # LLM Judge puanƒ±
                llm_result = self.evaluate_single(
                    question=result.get('question', ''),
                    ideal_answer=result.get('ideal_answer', ''),
                    model_answer=result.get('answer', '')
                )
                llm_score = llm_result.get('score', 0)
                
                # Primary metric puanƒ±
                primary_score = result.get('final_score', 0)
                
                # Farkƒ± kaydet
                diff = abs(llm_score - primary_score)
                correlations.append({
                    'question_id': idx,
                    'llm_score': llm_score,
                    'primary_score': primary_score,
                    'diff': diff
                })
                
                print(f"   Soru {idx+1}: LLM={llm_score:.1f}, Primary={primary_score:.1f}, Diff={diff:.1f}")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Soru {idx+1} deƒüerlendirilemedi: {e}")
                continue
        
        if len(correlations) < 2:
            print("‚ö†Ô∏è Yeterli sample deƒüerlendirilemedi")
            return {
                'avg_diff': 0.0,
                'max_diff': 0.0,
                'correlation': 0.0,
                'p_value': 1.0,
                'sample_size': len(correlations),
                'details': correlations
            }
        
        # ƒ∞statistikler
        avg_diff = sum(c['diff'] for c in correlations) / len(correlations)
        max_diff = max(c['diff'] for c in correlations)
        
        # Pearson correlation
        correlation = 0.0
        p_value = 1.0
        
        if SCIPY_AVAILABLE and len(correlations) >= 2:
            try:
                llm_scores = [c['llm_score'] for c in correlations]
                primary_scores = [c['primary_score'] for c in correlations]
                correlation, p_value = pearsonr(llm_scores, primary_scores)
            except Exception as e:
                print(f"‚ö†Ô∏è Korelasyon hesaplama hatasƒ±: {e}")
        
        print(f"\nüìä KORELASYON ANALƒ∞Zƒ∞:")
        print(f"   Ortalama Fark: {avg_diff:.1f} puan")
        print(f"   Maksimum Fark: {max_diff:.1f} puan")
        print(f"   Pearson r: {correlation:.3f} (p={p_value:.4f})")
        
        if correlation > 0.85:
            print(f"   ‚úÖ √áOK Y√úKSEK korelasyon! Primary metrics g√ºvenilir.")
        elif correlation > 0.70:
            print(f"   ‚úÖ ƒ∞Yƒ∞ korelasyon. Primary metrics kullanƒ±labilir.")
        elif correlation > 0.50:
            print(f"   ‚ö†Ô∏è ORTA korelasyon. Daha fazla sample gerekebilir.")
        else:
            print(f"   ‚ùå D√ú≈û√úK korelasyon! Primary metrics'i g√∂zden ge√ßir.")
        
        return {
            'avg_diff': avg_diff,
            'max_diff': max_diff,
            'correlation': correlation,
            'p_value': p_value,
            'sample_size': len(correlations),
            'details': correlations
        }
    
    def is_available(self) -> bool:
        """Judge kullanƒ±labilir mi?"""
        return self.available
